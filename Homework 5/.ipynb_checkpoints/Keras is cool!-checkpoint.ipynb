{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Keras is a neural network framework that wraps tensorflow (if you haven't heard of tensorflow it's another neural network framework) and makes it really simple to implement common neural networks. It's philosophy is to make simple things easy (but beware trying to implement uncommon, custom neural networks can be pretty challenging in Keras, for the purposes of this course you will never have to that though so don't worry about it). If you are ever confused during this homework, Keras has really good documentation, so just go to [Keras Docs](https://keras.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "Keras has many datasets conviently builtin to the library. We can access them from the ``keras.datasets`` module. For this homework, we will be using their housing price dataset, their image classification dataset and their movie review sentiment dataset. To get a full list of their datasets, you can go to this link. [Keras Datasets](https://keras.io/datasets). To use their datasets, we just import them and then call ``load_data()``, load_data returns two tuples, the first one is training data, and the second one is testing data. See the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose the proportion of training data you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set before:  (404, 13)\n",
      "Size of training set after:  (455, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of training set before: \", x_train.shape)\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data(test_split=0.10)\n",
    "print(\"Size of training set after: \", x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import normalize\n",
    "x_train = normalize(x_train, axis=1)\n",
    "x_test = normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Every thing in Keras starts out with a model. From an initial model, we can add layers, train the model on data, evaluate the model on test sets, etc. We initialize a model with ``Sequential()``. Sequential refers to the fact that the model has a sequence of layers. Personally, I have very rarely used anything other than sequential, so I think its all you really need to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a model, we can add layers to it with ``model.add``. Keras has a really good range of layers we can use. For example, if we want a basic fully connected layer we can use ``Dense``. I will now run through an example of using Keras to build and train a fully connected neural network for the purposes of regressing on housing prices for the dataset we loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "model.add(Dense(16, input_shape=(13,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code adds a fully connected layer with 32 neurons. For the first layer of any model we always have to specify the input shape. In our case we will be training a fully connected network on the boston housing data, so each data point has 13 features. That's why we use an input_shape of (13,). The nice part about Keras is other than the input_shape for the first layer, we don't have to worry about shapes the rest of the time, Keras takes care of it. This can be really useful when you are doing complicated convolutions and things like that where working out the input shape to the next layer can be non-trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add an Activation function to our network after our first fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple as that. We just added a relu activation to the whole layer. To see a list of activation functions available in Keras go to [Keras Activations](https://keras.io/activations/). Now let's add the final layer in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use a handy utility in Keras to print out what our model looks like so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                224       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 241\n",
      "Trainable params: 241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see it shows us what layers we have, the output shapes of each layer, and how many parameters there are for each layer. All this information can be really useful when trying to debug a model, or even for sharing your model architechture with others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Now for actually training the model. Before we train a model we have to compile it. ``model.compile`` is how you specify which optimizer to use and what loss function to use. Sometimes choosing the right optimizer can have a significant effect on model performance. For a list of optimizers look at [Keras Optimizers](https://keras.io/optimizers). Choosing the right optimizer is mostly just trying each one to see which works better, there is some general advice for when to use each one but its basically just another hyperparameter. We also have to choose a loss function. Choosing the right loss function is really important since the loss function basically decides what the goal of the model is. Since we are doing regression we want to choose mean squared error, to get our output to be as close as possible to the label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='SGD', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to actually train our model on the data. This is really easy in Keras, in fact it only takes one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "455/455 [==============================] - 0s 191us/step - loss: 222.0887\n",
      "Epoch 2/100\n",
      "455/455 [==============================] - 0s 33us/step - loss: 70.9517\n",
      "Epoch 3/100\n",
      "455/455 [==============================] - 0s 52us/step - loss: 68.6349\n",
      "Epoch 4/100\n",
      "455/455 [==============================] - 0s 50us/step - loss: 68.1223\n",
      "Epoch 5/100\n",
      "455/455 [==============================] - 0s 55us/step - loss: 65.0592\n",
      "Epoch 6/100\n",
      "455/455 [==============================] - 0s 58us/step - loss: 62.7599\n",
      "Epoch 7/100\n",
      "455/455 [==============================] - 0s 44us/step - loss: 70.0541\n",
      "Epoch 8/100\n",
      "455/455 [==============================] - 0s 67us/step - loss: 63.9302\n",
      "Epoch 9/100\n",
      "455/455 [==============================] - 0s 66us/step - loss: 68.1515\n",
      "Epoch 10/100\n",
      "455/455 [==============================] - 0s 52us/step - loss: 67.3017\n",
      "Epoch 11/100\n",
      "455/455 [==============================] - 0s 56us/step - loss: 68.4557\n",
      "Epoch 12/100\n",
      "455/455 [==============================] - 0s 51us/step - loss: 63.9498\n",
      "Epoch 13/100\n",
      "455/455 [==============================] - 0s 58us/step - loss: 64.7202\n",
      "Epoch 14/100\n",
      "455/455 [==============================] - 0s 46us/step - loss: 62.9871\n",
      "Epoch 15/100\n",
      "455/455 [==============================] - 0s 49us/step - loss: 65.0702\n",
      "Epoch 16/100\n",
      "455/455 [==============================] - 0s 41us/step - loss: 62.3367\n",
      "Epoch 17/100\n",
      "455/455 [==============================] - 0s 49us/step - loss: 64.3243\n",
      "Epoch 18/100\n",
      "455/455 [==============================] - 0s 52us/step - loss: 63.5915\n",
      "Epoch 19/100\n",
      "455/455 [==============================] - 0s 37us/step - loss: 63.2361\n",
      "Epoch 20/100\n",
      "455/455 [==============================] - 0s 50us/step - loss: 65.5026\n",
      "Epoch 21/100\n",
      "455/455 [==============================] - 0s 43us/step - loss: 63.4076\n",
      "Epoch 22/100\n",
      "455/455 [==============================] - 0s 58us/step - loss: 61.5316\n",
      "Epoch 23/100\n",
      "455/455 [==============================] - 0s 46us/step - loss: 63.8373\n",
      "Epoch 24/100\n",
      "455/455 [==============================] - 0s 41us/step - loss: 61.3128\n",
      "Epoch 25/100\n",
      "455/455 [==============================] - 0s 55us/step - loss: 60.8185\n",
      "Epoch 26/100\n",
      "455/455 [==============================] - 0s 38us/step - loss: 61.5479\n",
      "Epoch 27/100\n",
      "455/455 [==============================] - 0s 65us/step - loss: 62.9179\n",
      "Epoch 28/100\n",
      "455/455 [==============================] - 0s 97us/step - loss: 63.2571\n",
      "Epoch 29/100\n",
      "455/455 [==============================] - 0s 187us/step - loss: 59.3105\n",
      "Epoch 30/100\n",
      "455/455 [==============================] - 0s 103us/step - loss: 62.6409\n",
      "Epoch 31/100\n",
      "455/455 [==============================] - 0s 110us/step - loss: 58.6116\n",
      "Epoch 32/100\n",
      "455/455 [==============================] - 0s 77us/step - loss: 64.4478\n",
      "Epoch 33/100\n",
      "455/455 [==============================] - 0s 57us/step - loss: 62.9598\n",
      "Epoch 34/100\n",
      "455/455 [==============================] - 0s 62us/step - loss: 61.8426\n",
      "Epoch 35/100\n",
      "455/455 [==============================] - 0s 56us/step - loss: 61.7504\n",
      "Epoch 36/100\n",
      "455/455 [==============================] - 0s 36us/step - loss: 62.8151\n",
      "Epoch 37/100\n",
      "455/455 [==============================] - 0s 67us/step - loss: 59.2473\n",
      "Epoch 38/100\n",
      "455/455 [==============================] - 0s 49us/step - loss: 59.0279\n",
      "Epoch 39/100\n",
      "455/455 [==============================] - 0s 53us/step - loss: 60.4624\n",
      "Epoch 40/100\n",
      "455/455 [==============================] - 0s 49us/step - loss: 60.3982\n",
      "Epoch 41/100\n",
      "455/455 [==============================] - 0s 50us/step - loss: 59.2849\n",
      "Epoch 42/100\n",
      "455/455 [==============================] - 0s 51us/step - loss: 58.5716\n",
      "Epoch 43/100\n",
      "455/455 [==============================] - 0s 46us/step - loss: 59.2411\n",
      "Epoch 44/100\n",
      "455/455 [==============================] - 0s 57us/step - loss: 59.3925\n",
      "Epoch 45/100\n",
      "455/455 [==============================] - 0s 45us/step - loss: 59.5439\n",
      "Epoch 46/100\n",
      "455/455 [==============================] - 0s 42us/step - loss: 60.6901\n",
      "Epoch 47/100\n",
      "455/455 [==============================] - 0s 56us/step - loss: 61.3533\n",
      "Epoch 48/100\n",
      "455/455 [==============================] - 0s 56us/step - loss: 57.9314\n",
      "Epoch 49/100\n",
      "455/455 [==============================] - 0s 91us/step - loss: 59.4361\n",
      "Epoch 50/100\n",
      "455/455 [==============================] - 0s 101us/step - loss: 57.5553\n",
      "Epoch 51/100\n",
      "455/455 [==============================] - 0s 73us/step - loss: 64.3022\n",
      "Epoch 52/100\n",
      "455/455 [==============================] - 0s 96us/step - loss: 61.4032\n",
      "Epoch 53/100\n",
      "455/455 [==============================] - 0s 77us/step - loss: 61.2102\n",
      "Epoch 54/100\n",
      "455/455 [==============================] - 0s 60us/step - loss: 59.3129\n",
      "Epoch 55/100\n",
      "455/455 [==============================] - 0s 52us/step - loss: 59.3812\n",
      "Epoch 56/100\n",
      "455/455 [==============================] - 0s 81us/step - loss: 59.1564\n",
      "Epoch 57/100\n",
      "455/455 [==============================] - 0s 64us/step - loss: 65.7299\n",
      "Epoch 58/100\n",
      "455/455 [==============================] - 0s 73us/step - loss: 58.1555\n",
      "Epoch 59/100\n",
      "455/455 [==============================] - 0s 68us/step - loss: 59.0042\n",
      "Epoch 60/100\n",
      "455/455 [==============================] - 0s 72us/step - loss: 56.6999\n",
      "Epoch 61/100\n",
      "455/455 [==============================] - 0s 53us/step - loss: 59.8462\n",
      "Epoch 62/100\n",
      "455/455 [==============================] - 0s 66us/step - loss: 57.0653\n",
      "Epoch 63/100\n",
      "455/455 [==============================] - 0s 70us/step - loss: 61.5052\n",
      "Epoch 64/100\n",
      "455/455 [==============================] - 0s 95us/step - loss: 59.3374\n",
      "Epoch 65/100\n",
      "455/455 [==============================] - 0s 106us/step - loss: 57.4246\n",
      "Epoch 66/100\n",
      "455/455 [==============================] - 0s 90us/step - loss: 56.9533\n",
      "Epoch 67/100\n",
      "455/455 [==============================] - 0s 75us/step - loss: 57.3594\n",
      "Epoch 68/100\n",
      "455/455 [==============================] - 0s 50us/step - loss: 57.5222\n",
      "Epoch 69/100\n",
      "455/455 [==============================] - 0s 63us/step - loss: 59.7531\n",
      "Epoch 70/100\n",
      "455/455 [==============================] - 0s 59us/step - loss: 62.2567\n",
      "Epoch 71/100\n",
      "455/455 [==============================] - 0s 71us/step - loss: 60.5483\n",
      "Epoch 72/100\n",
      "455/455 [==============================] - 0s 63us/step - loss: 61.0745\n",
      "Epoch 73/100\n",
      "455/455 [==============================] - 0s 72us/step - loss: 61.3684\n",
      "Epoch 74/100\n",
      "455/455 [==============================] - 0s 115us/step - loss: 57.5464\n",
      "Epoch 75/100\n",
      "455/455 [==============================] - 0s 40us/step - loss: 61.2874\n",
      "Epoch 76/100\n",
      "455/455 [==============================] - 0s 35us/step - loss: 65.5524\n",
      "Epoch 77/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 63.0956\n",
      "Epoch 78/100\n",
      "455/455 [==============================] - 0s 48us/step - loss: 55.3741\n",
      "Epoch 79/100\n",
      "455/455 [==============================] - 0s 71us/step - loss: 76.0759\n",
      "Epoch 80/100\n",
      "455/455 [==============================] - 0s 69us/step - loss: 57.7559\n",
      "Epoch 81/100\n",
      "455/455 [==============================] - 0s 81us/step - loss: 56.0875\n",
      "Epoch 82/100\n",
      "455/455 [==============================] - 0s 115us/step - loss: 57.5838\n",
      "Epoch 83/100\n",
      "455/455 [==============================] - 0s 118us/step - loss: 58.8828\n",
      "Epoch 84/100\n",
      "455/455 [==============================] - 0s 47us/step - loss: 59.0534\n",
      "Epoch 85/100\n",
      "455/455 [==============================] - 0s 77us/step - loss: 56.4800\n",
      "Epoch 86/100\n",
      "455/455 [==============================] - 0s 85us/step - loss: 57.6438\n",
      "Epoch 87/100\n",
      "455/455 [==============================] - 0s 73us/step - loss: 56.8578\n",
      "Epoch 88/100\n",
      "455/455 [==============================] - 0s 77us/step - loss: 57.4104\n",
      "Epoch 89/100\n",
      "455/455 [==============================] - 0s 52us/step - loss: 56.9132\n",
      "Epoch 90/100\n",
      "455/455 [==============================] - 0s 39us/step - loss: 56.9408\n",
      "Epoch 91/100\n",
      "455/455 [==============================] - 0s 40us/step - loss: 61.6062\n",
      "Epoch 92/100\n",
      "455/455 [==============================] - 0s 46us/step - loss: 55.1484\n",
      "Epoch 93/100\n",
      "455/455 [==============================] - 0s 44us/step - loss: 56.7093\n",
      "Epoch 94/100\n",
      "455/455 [==============================] - 0s 80us/step - loss: 56.8680\n",
      "Epoch 95/100\n",
      "455/455 [==============================] - 0s 76us/step - loss: 57.2416\n",
      "Epoch 96/100\n",
      "455/455 [==============================] - 0s 290us/step - loss: 59.0445\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 0s 88us/step - loss: 61.5496\n",
      "Epoch 98/100\n",
      "455/455 [==============================] - 0s 65us/step - loss: 56.6806\n",
      "Epoch 99/100\n",
      "455/455 [==============================] - 0s 64us/step - loss: 57.6510\n",
      "Epoch 100/100\n",
      "455/455 [==============================] - 0s 66us/step - loss: 59.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e49e400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Now that we have trained our model we can evaluate it on our testing set. It is also just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  77.7915217082\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \", model.evaluate(x_test, y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss might seem very high and it is, mostly because there aren't very many training points in the dataset (also no effort was put into finding the best model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate predictions for new data that we don't have labels for. Since we don't have new data, I will just demonstrate the idea with our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.48513222]\n",
      " [ 17.21039581]\n",
      " [ 21.25908852]\n",
      " [ 24.36770821]\n",
      " [ 24.08263397]\n",
      " [ 17.37683105]\n",
      " [ 24.74196434]\n",
      " [ 27.49230576]\n",
      " [ 23.69055939]\n",
      " [ 17.77512169]\n",
      " [ 17.48779106]\n",
      " [ 13.67065048]\n",
      " [ 21.00523567]\n",
      " [ 23.24888229]\n",
      " [ 28.07192993]\n",
      " [ 15.96839523]\n",
      " [ 26.91202736]\n",
      " [ 15.50911713]\n",
      " [ 17.02207756]\n",
      " [ 17.74651146]\n",
      " [ 24.63841057]\n",
      " [ 17.82217026]\n",
      " [ 16.14072037]\n",
      " [ 24.91880608]\n",
      " [ 22.77306366]\n",
      " [ 23.21960068]\n",
      " [ 23.40651131]\n",
      " [ 27.21398735]\n",
      " [ 16.5730629 ]\n",
      " [ 22.06626511]\n",
      " [ 24.39781761]\n",
      " [ 22.73048782]\n",
      " [ 17.31253624]\n",
      " [ 22.21966362]\n",
      " [ 22.20745468]\n",
      " [ 17.55694199]\n",
      " [ 22.00107574]\n",
      " [ 22.88581276]\n",
      " [ 18.17415237]\n",
      " [ 23.05173111]\n",
      " [ 25.00395203]\n",
      " [ 24.63780403]\n",
      " [ 24.78661156]\n",
      " [ 27.38204575]\n",
      " [ 19.71298218]\n",
      " [ 24.08311653]\n",
      " [ 17.58635902]\n",
      " [ 21.98981285]\n",
      " [ 20.71031761]\n",
      " [ 22.12649727]\n",
      " [ 16.9145546 ]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(x_test)\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We have successfully (depending on your definition of success) built a fully connected neural network and trained that network on a dataset. Now its your turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 1: Image Classification\n",
    "We are going to build a convolutional neural network to predict image classes on CIFAR-10, a dataset of images of 10 different things (i.e. 10 classes). Things like aeroplanes, cars, deer, horses, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Load the cifar10 dataset from Keras. If you need a hint go to [Keras Datasets](https://keras.io/datasets). This might take a little while to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(cifar_x_train, cifar_y_train), (cifar_x_test, cifar_y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Initialize a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Add a ``Conv2D`` layer to the model. It should have 32 filters, a 5x5 kernel, and a 1x1 stride. The documentation [here](https://keras.io/layers/convolutional/#conv2d) will be your friend for this problem. __Hint:__ This is the first layer of the model so you have to specify the input shape. I recommend printing ``cifar_x_train.shape``, to get an idea of what the shape of the data looks like. Then add a ```relu``` activation layer to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional import Conv2D\n",
    "print(cifar_x_train.shape)\n",
    "#YOUR CODE HERE\n",
    "cifar_model.add(Conv2D(32, (5,5), strides=(1, 1), input_shape=(32, 32, 3)))\n",
    "cifar_model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Add a ``MaxPooling2D`` layer to the model. The layer should have a 2x2 pool size. The documentation for Max Pooling is [here](https://keras.io/layers/pooling/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.pooling import MaxPooling2D\n",
    "##YOUR CODE HERE\n",
    "cifar_model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Add another ``Conv2D`` identical to last one, then another ``relu`` activation, then another ``MaxPooling2D`` layer. __Hint:__ You've already written this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE HERE\n",
    "cifar_model.add(Conv2D(32, (5,5), strides=(1, 1)))\n",
    "cifar_model.add(Activation('relu'))\n",
    "cifar_model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f)** Add another ``Conv2D`` layer identical to the others except with 64 filters instead of 32. Add another ``relu`` activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE HERE\n",
    "cifar_model.add(Conv2D(64, (5,5), strides=(1, 1)))\n",
    "cifar_model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g)** Now we want to move from 2D data to 1D vectors for classification, to this we have to flatten the data. Keras has a layer for this called [Flatten](https://keras.io/layers/core/#flatten). Then add a ``Dense`` (fully connected) layer with 64 neurons, a ``relu`` activation layer, another ``Dense`` layer with 10 neurons, and a ``softmax`` activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1259: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "##YOUR CODE HERE\n",
    "cifar_model.add(Flatten())\n",
    "cifar_model.add(Dense(64))\n",
    "cifar_model.add(Activation('relu'))\n",
    "cifar_model.add(Dense(10))\n",
    "cifar_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have constructed a network that takes in an image and outputs a vector of 10 numbers and then we take the softmax of these, which leaves us we a vector of 0s except 1 one and the location of this one in the vector corresponds to which class the network is predicting for that image. This is sort of the canonical way of doing image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h)** Now print a summary of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        2432      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 32)        25632     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 1, 64)          51264     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 84,138\n",
      "Trainable params: 84,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##YOUR CODE HERE\n",
    "cifar_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i)** We need to convert our labels from integers to length 10 vectors with 9 zeros and 1 one, where the integer label is the index of the 1 in the vector. Luckily, Keras has a handy function to do this for us. Have a look [here](https://keras.io/utils/#to_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train_cat = ???\n",
    "y_test_cat = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(j)** Now compile the model with SGD optimizer and categorical_crossentropy loss function, also include ``metrics=['accuracy']`` as a parameter so we can see the accuracy of the model. Then train the model on the training data. For training we want to weight the classes in the loss function, so set the ``class_weight`` parameter of fit to be the ``class_weights`` dictionary. Be warned training can take forever, I trained on a cpu for 20 epochs (about 30 minutes) and only got 20% accuracy. For the purposes of this assignment you don't need to worry to much about accuracy, just train for at least 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR COMPILING CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "for i in range(10):\n",
    "    class_weights[i] = 1. / np.where(cifar_y_train==i)[0].size\n",
    "\n",
    "##YOUR TRAINING CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_model.evaluate(cifar_x_test, y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the class labels the network predicts on our test set and look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cifar_model.predict(cifar_x_test)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(cifar_x_test[1234])\n",
    "print(\"Predicted label: \", np.argmax(y_pred[1234]))\n",
    "print(\"True label: \", cifar_y_test[1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will use Kera's imdb sentiment dataset. You will take in sequences of words and use an RNN to try to classify the sequences sentiment. First we have to process the data a little bit, so that we have fixed length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    processed = np.zeros(len(data) * 200).reshape((len(data), 200))\n",
    "    for i, seq in enumerate(data):\n",
    "        if len(seq) < 200:\n",
    "            processed[i] = np.array(seq + [0 for _ in range(200 - len(seq))])\n",
    "        else:\n",
    "            processed[i] = np.array(seq)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_proc = process_data(x_train)\n",
    "x_test_proc = process_data(x_test)\n",
    "print(x_test_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding Layer is a little bit different from most of the layers, so we have provided that code for you. Basically, the 1000 means that we are using a vocabulary size of 1000, the 32 means we will have a vector of size 32 outputed, and the mask zero means that we don't care about 0, because we are using it for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "imdb_model.add(Embedding(1000, 32, input_length=200, mask_zero=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** For this problem, I won't walk you everything like I did in the last one. What you need to do is as follows. Add an LSTM layer with 32 outputs, then a Dense layer with 32 neurons, then a relu activation, then a dense layer with 1 neuron, then a sigmoid activation. Then you should print out the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Now compile the model with binary cross entropy, and the adam optimizer. Also include accuracy as a metric in the compile. Then train the model on the processed data (no need to worry about class weights this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we can evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", imdb_model.evaluate(x_test_proc, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at our predictions and the sentences they correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = imdb_model.predict(x_test_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.vectorize(lambda x: int(x >= 0.5))(y_pred)\n",
    "correct = []\n",
    "incorrect = []\n",
    "for i, pred in enumerate(y_pred):\n",
    "    if y_test[i] == pred:\n",
    "        correct.append(i)\n",
    "    else:\n",
    "        incorrect.append(i)\n",
    "word_dict = inv_map = {v: k for k, v in imdb.get_word_index().items()}\n",
    "\n",
    "print(list(map(lambda x: word_dict[int(x)] if x != 0 else None, x_test[correct[123]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making this I realized that keras' method for converting from word index back to words is broken right now (see this open [github issue](https://github.com/fchollet/keras/issues/5912)). So we can't actually see what the sentences look like."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
